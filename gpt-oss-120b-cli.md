# Deploying Local LLMs on an M1â€¯16â€¯GB MacBook (and scaling to 128â€¯GB)

Below is a **complete, copyâ€‘andâ€‘paste ready** guide that takes you from a fresh macOS install to a fully functional local LLM stack using **Ollama** as the core engine.  
It also covers optional UI/CLI tools, modelâ€‘sharing with **gollama**, and how the stack changes (or doesnâ€™t) when you upgrade to a 128â€¯GB machine.

---

## 1ï¸âƒ£ Prerequisites

| Requirement | Details |
|------------|---------|
| macOS version | 11â€¯Bigâ€¯Sur or later (M1â€¯MacBookâ€¯Proâ€¯2020 works fine) |
| RAM | 16â€¯GB (minimum for 7â€‘13â€¯B models) â€“ 128â€¯GB lets you run 34â€¯Bâ€‘70â€¯B models with quantization |
| Disk | â‰¥â€¯8â€¯GB free for a 7â€¯B model; larger models need proportionally more space |
| Admin rights | Needed for Homebrew, Docker, and port configuration |
| Optional (UI) | Docker Desktop for Appleâ€¯Silicon (to run Openâ€¯WebUI) |
| Optional (IDE) | VSâ€¯Code + **Continue** or **Openâ€‘Interpreter** for code assistance |

*Sources: Ollama hardware guide, Apple Silicon performance tables*ã€Sourceã€‘.

---

## 2ï¸âƒ£ Install the Core Stack

### 2.1 Install Homebrew (if you donâ€™t have it)

```bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

### 2.2 Install Ollama

```bash
brew install ollama          # CLI + background service
brew services start ollama   # start Ollama as a macOS service
```

*You can also use the oneâ€‘liner `curl https://ollama.ai/install.sh | sh`*ã€Sourceã€‘.

### 2.3 Verify the installation

```bash
ollama --version
```

You should see something like `0.12.10` (or newer).

---

## 3ï¸âƒ£ Pull & Run a Model (CLIâ€‘first workflow)

> **Tip:** Use a quantized version (`q4_0` or `q8_0`) to fit into 16â€¯GB RAM.

```bash
# Example: 7â€‘B Llamaâ€¯3.1 quantized to 4â€‘bit
ollama pull llama3.1:8b-q4_0
```

Start an interactive session:

```bash
ollama run llama3.1:8b-q4_0
```

Youâ€™ll get a REPL prompt (`>>>`) where you can type questions.

### 3.1 Adjust runtime parameters (optional)

```bash
# Set max tokens
/set parameter num_predict 512

# Set temperature (creativity)
/set parameter temperature 0.7

# Set context length (tokens)
#set parameter num_ctx 2048
```

*All parameters are documented in the Ollama CLI reference*ã€Sourceã€‘.

---

## 4ï¸âƒ£ Adding a Friendly UI (optional but highly recommended)

### 4.1 Install Docker Desktop (Appleâ€¯Silicon build)

Download from <https://docs.docker.com/desktop/install/mac-install/> and run the installer.

### 4.2 Run Openâ€¯WebUI (selfâ€‘hosted chat UI)

```bash
docker run -d \
  -p 3000:8080 \
  -v open-webui:/app/backend/data \
  --add-host=host.docker.internal:host-gateway \
  ghcr.io/open-webui/open-webui:main
```

Open `http://localhost:3000` in a browser, create an admin account, then:

1. **Settings â†’ Backend** â†’ select **Ollama**.  
2. **API endpoint**: `http://localhost:11434` (default).  
3. **Test connection** â†’ should succeed.  

Now you can pick any installed Ollama model from the dropdown and chat via a GUI.

*Openâ€¯WebUI works outâ€‘ofâ€‘theâ€‘box with Ollama*ã€Sourceã€‘.

---

## 5ï¸âƒ£ Integrating with Development Tools

### 5.1 VSâ€¯Code + Continue (local codeâ€‘assistant)

1. Install the **Continue** extension from the VSâ€¯Code Marketplace.  
2. In `continue.json` (or UI settings) add:

```json
{
  "models": [
    {
      "title": "Llamaâ€¯3.1â€¯8B",
      "model": "llama3.1:8b-q4_0",
      "provider": "ollama",
      "apiBase": "http://localhost:11434"
    }
  ],
  "tabAutocompleteModel": {
    "title": "Qwen2â€‘Coderâ€¯1.5B",
    "model": "qwen2.5-coder:1.5b",
    "provider": "ollama",
    "apiBase": "http://localhost:11434"
  }
}
```

Now you get inline autocomplete and a chat sidebar powered entirely locally.

*See the â€œAI coding assistant in VSâ€¯Codeâ€ guide*ã€Sourceã€‘.

### 5.2 Openâ€‘Interpreter (CLIâ€¯+â€¯fileâ€‘system)

```bash
pip install open-interpreter
export OLLAMA_HOST=http://localhost:11434   # optional, defaults to this
interpreter
```

You can now run commands like `ls`, `cat`, or even edit files; the interpreter calls Ollama via its REST API (`/api/generate`).

*Openâ€‘Interpreter works with any Ollama model*ã€Sourceã€‘.

---

## 6ï¸âƒ£ Sharing / Distributing Models (gollama)

-  **gollama** is a small Go utility that lets you **push/pull** Ollama model â€œimagesâ€ to a shared registry (e.g., a private Git repo or an internal HTTP server).  
-  Use it **only if you need to distribute a customâ€‘built model** across multiple machines.  
-  For a singleâ€‘machine workflow you can skip it entirely and rely on `ollama pull`.

```bash
# Publish a local model
gollama push myâ€‘fineâ€‘tunedâ€‘model

# Pull on another Mac
gollama pull myâ€‘fineâ€‘tunedâ€‘model
```

*The gollama repo lists it as the official way to share Ollama images*ã€Sourceã€‘.

---

## 7ï¸âƒ£ Scaling to a 128â€¯GB Machine

| RAM | Practical model size (quantized) | Typical useâ€‘case |
|-----|----------------------------------|------------------|
| 16â€¯GB | 7â€‘13â€¯B (Q4) | General chat, small code assistance |
| 32â€¯GB | 13â€‘34â€¯B (Q4/Q8) | More nuanced reasoning, longer context |
| 64â€‘128â€¯GB | 34â€‘70â€¯B (Q4) or 70â€‘110â€¯B (Q8) | Complex multiâ€‘step tasks, RAG with large knowledge bases |
| 192â€¯GB (Macâ€¯Studio) | 110â€¯B+ (4â€‘bit) | Researchâ€‘grade LLMs, MoE models (Mixtral) |

**What changes?**

1. **Model choice** â€“ you can now pull larger models (`llama3.2:34b-q4_0`, `llama4:70b-q4_0`, etc.).  
2. **No stack change** â€“ the same `ollama`, `docker`, `open-webui`, `continue`, `open-interpreter` tools work unchanged.  
3. **Resource tuning** â€“ increase `num_ctx` (e.g., 8192 tokens) and optionally disable swap (`launchctl limit maxfiles â€¦`) for smoother performance.  
4. **Optional multiâ€‘model serving** â€“ with abundant RAM you can run several models simultaneously (`ollama serve & ollama run â€¦ & â€¦`) and route different tasks (code vs. prose) to dedicated models.

*Hardwareâ€‘model mapping from the Ollama documentation*ã€Sourceã€‘.

---

## 8ï¸âƒ£ Full Endâ€‘toâ€‘End Script (copyâ€‘paste)

```bash
# 1ï¸âƒ£ Install Homebrew (skip if already installed)
#!/usr/bin/env bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# 2ï¸âƒ£ Install Ollama & start as a service
brew install ollama
brew services start ollama

# 3ï¸âƒ£ Verify
ollama --version

# 4ï¸âƒ£ Pull a 7B quantized model (fits 16â€¯GB)
ollama pull llama3.1:8b-q4_0

# 5ï¸âƒ£ Test CLI
ollama run llama3.1:8b-q4_0 <<'EOF'
What is the capital of Spain?
EOF

# 6ï¸âƒ£ (Optional) Install Docker Desktop manually from Docker website

# 7ï¸âƒ£ Run Open WebUI UI
docker run -d \
  -p 3000:8080 \
  -v open-webui:/app/backend/data \
  --add-host=host.docker.internal:host-gateway \
  ghcr.io/open-webui/open-webui:main

# 8ï¸âƒ£ Open http://localhost:3000, configure backend â†’ Ollama â†’ http://localhost:11434

# 9ï¸âƒ£ (Optional) VS Code Continue config â€“ add the JSON snippet from section 5.1

# 10ï¸âƒ£ (Optional) Install Openâ€‘Interpreter
pip install open-interpreter
export OLLAMA_HOST=http://localhost:11434
interpreter
```

Run the script lineâ€‘byâ€‘line (or paste into a terminal). After stepâ€¯8 you have a full web chat; after stepâ€¯9 you have IDE autocomplete; after stepâ€¯10 you can script fileâ€‘system operations.

---

## 9ï¸âƒ£ Recommendations & Best Practices

| Goal | Recommended Stack |
|------|-------------------|
| **Pure CLI workflow** | Ollama CLI only (`ollama run â€¦`) â€“ fastest, no extra dependencies |
| **Web UI for occasional use** | Ollama + Dockerâ€‘run Openâ€¯WebUI |
| **IDE code assistance** | Ollama + VSâ€¯Code **Continue** (or **aichat**) |
| **Fileâ€‘system automation** | Openâ€‘Interpreter (calls Ollama via REST) |
| **Sharing custom models across a team** | Use **gollama** to push/pull model images |
| **Scaling to 128â€¯GB** | Same stack; just pull larger models and increase `num_ctx`/`temperature` as needed |

*All recommendations stem from the collective community guides and official docs*ã€Sourceã€‘.

---

### ğŸ‰ Youâ€™re ready!

-  **Run `ollama list`** to see installed models.  
-  **Use `ollama rm <model>`** to free space when you need to test a larger model.  
-  **Monitor memory** with Activity Monitor; stay ~2â€¯GB below total RAM to keep macOS responsive.  

Enjoy a **private, offline AI** that runs directly on your Mâ€‘series Macâ€”no cloud, no API keys, full control over data and model versions.  

---  

*All commands are ready to be copied into your terminal or markdown editor.*
