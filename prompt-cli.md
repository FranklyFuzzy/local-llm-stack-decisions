"Can you provide a detailed, step-by-step guide on how to deploy local LLMs on my M1 16GB MacBook from scratch? I want to use Ollama as the primary LLM and need assistance deciding which stack to use for deployment that supports CLI tooling and host file system interaction. Should I use Ollama to pull images? Gollama to share LLM images with CLI integration tools like OpenCode or Open-Interpreter? Or should I focus on implementing a CLI tool that integrates directly with Ollama? How is this impacted if I get a machine with 128GB, outside of model recomentation changing, any change to the stack? format this output into a clean md format I can copy it into my editor of choice."
